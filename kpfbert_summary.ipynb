{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytorch-lightning) (1.26.0)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytorch-lightning) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2023.10.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
      "  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytorch-lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytorch-lightning) (4.8.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch>=1.12.0->pytorch-lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch>=1.12.0->pytorch-lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch>=1.12.0->pytorch-lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n",
      "Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
      "   ---------------------------------------- 0.0/776.9 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 61.4/776.9 kB 3.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 337.9/776.9 kB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 583.7/776.9 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  768.0/776.9 kB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 776.9/776.9 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
      "   ---------------------------------------- 0.0/806.1 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 204.8/806.1 kB 13.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 368.6/806.1 kB 5.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 368.6/806.1 kB 5.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 368.6/806.1 kB 5.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 675.8/806.1 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  788.5/806.1 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 806.1/806.1 kB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
      "Successfully installed lightning-utilities-0.10.0 pytorch-lightning-2.1.2 torchmetrics-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-4.5.4.tar.gz (79 kB)\n",
      "     ---------------------------------------- 0.0/79.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/79.1 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/79.1 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 41.0/79.1 kB 393.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 79.1/79.1 kB 550.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting emoji==1.2.0 (from kss)\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "     ---------------------------------------- 0.0/131.3 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 122.9/131.3 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 131.3/131.3 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from kss) (2023.10.3)\n",
      "Collecting pecab (from kss)\n",
      "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
      "     ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.3/26.4 MB 6.3 MB/s eta 0:00:05\n",
      "      --------------------------------------- 0.6/26.4 MB 7.4 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.9/26.4 MB 7.7 MB/s eta 0:00:04\n",
      "     - -------------------------------------- 1.3/26.4 MB 7.3 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 1.6/26.4 MB 8.0 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 2.1/26.4 MB 8.0 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 2.5/26.4 MB 8.0 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 2.9/26.4 MB 8.0 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 3.3/26.4 MB 8.0 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 3.7/26.4 MB 8.2 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 4.1/26.4 MB 8.2 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 4.4/26.4 MB 8.2 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 4.7/26.4 MB 8.2 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 5.1/26.4 MB 8.1 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 5.3/26.4 MB 7.9 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 5.7/26.4 MB 7.9 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 5.8/26.4 MB 7.7 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 5.8/26.4 MB 7.7 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 6.3/26.4 MB 7.4 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 6.6/26.4 MB 7.3 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 6.9/26.4 MB 7.2 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 7.2/26.4 MB 7.2 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 7.6/26.4 MB 7.2 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 7.9/26.4 MB 7.3 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 7.9/26.4 MB 7.3 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 7.9/26.4 MB 7.3 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 7.9/26.4 MB 7.3 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 7.9/26.4 MB 7.3 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 8.4/26.4 MB 6.4 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 8.6/26.4 MB 6.4 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 8.8/26.4 MB 6.2 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 8.8/26.4 MB 6.1 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 9.0/26.4 MB 6.1 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 9.0/26.4 MB 6.1 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 9.0/26.4 MB 6.1 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 9.3/26.4 MB 5.7 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 9.5/26.4 MB 5.7 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 9.7/26.4 MB 5.6 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 9.9/26.4 MB 5.6 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 10.0/26.4 MB 5.6 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 10.0/26.4 MB 5.4 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 10.2/26.4 MB 5.3 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 10.4/26.4 MB 5.3 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 10.6/26.4 MB 5.3 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 10.9/26.4 MB 5.3 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 11.2/26.4 MB 5.2 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 11.5/26.4 MB 5.2 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 11.8/26.4 MB 5.2 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 12.2/26.4 MB 5.2 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 12.4/26.4 MB 5.1 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 12.7/26.4 MB 5.1 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 12.9/26.4 MB 5.1 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 13.2/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 13.5/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 13.8/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 14.2/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 14.5/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 14.9/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 15.2/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 15.5/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 15.8/26.4 MB 4.9 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 16.0/26.4 MB 5.2 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 16.2/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 16.2/26.4 MB 5.0 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 16.4/26.4 MB 4.8 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 16.4/26.4 MB 4.8 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 16.8/26.4 MB 4.7 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 17.1/26.4 MB 4.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 17.4/26.4 MB 4.8 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 17.8/26.4 MB 4.7 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 18.1/26.4 MB 5.2 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 18.4/26.4 MB 5.2 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 18.6/26.4 MB 5.1 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 18.9/26.4 MB 5.2 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 19.2/26.4 MB 5.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 19.4/26.4 MB 5.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 19.7/26.4 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 19.9/26.4 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 20.0/26.4 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 20.4/26.4 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 20.5/26.4 MB 5.8 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 20.8/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 21.0/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 21.4/26.4 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 21.6/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 22.0/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 22.1/26.4 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 22.4/26.4 MB 5.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 22.8/26.4 MB 5.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.1/26.4 MB 6.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.5/26.4 MB 6.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.6/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 24.1/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 24.4/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.7/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.9/26.4 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 25.4/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.8/26.4 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  26.0/26.4 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  26.4/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  26.4/26.4 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 26.4/26.4 MB 5.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: networkx in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from kss) (3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pecab->kss) (1.26.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pecab->kss) (14.0.1)\n",
      "Collecting pytest (from pecab->kss)\n",
      "  Downloading pytest-7.4.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting iniconfig (from pytest->pecab->kss)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytest->pecab->kss) (23.2)\n",
      "Collecting pluggy<2.0,>=0.12 (from pytest->pecab->kss)\n",
      "  Downloading pluggy-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytest->pecab->kss) (1.1.3)\n",
      "Collecting tomli>=1.0.0 (from pytest->pecab->kss)\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pytest->pecab->kss) (0.4.6)\n",
      "Downloading pytest-7.4.3-py3-none-any.whl (325 kB)\n",
      "   ---------------------------------------- 0.0/325.1 kB ? eta -:--:--\n",
      "   ---------------------------------------  317.4/325.1 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 325.1/325.1 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading pluggy-1.3.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: kss, pecab\n",
      "  Building wheel for kss (setup.py): started\n",
      "  Building wheel for kss (setup.py): finished with status 'done'\n",
      "  Created wheel for kss: filename=kss-4.5.4-py3-none-any.whl size=54780 sha256=eabc31379652100119fdaa72f7d9d7441454066c7769505b32217a69aaf96afb\n",
      "  Stored in directory: c:\\users\\echoi\\appdata\\local\\pip\\cache\\wheels\\f2\\ae\\3b\\a9fb20e80d5be0b3a552b3086f9c92c1675306ff7404f3d0a5\n",
      "  Building wheel for pecab (setup.py): started\n",
      "  Building wheel for pecab (setup.py): finished with status 'done'\n",
      "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646702 sha256=a46d3a64a8dc5a9c2205731933c442b4588e37dd7151c9dcf29ba79686ecad04\n",
      "  Stored in directory: c:\\users\\echoi\\appdata\\local\\pip\\cache\\wheels\\5c\\91\\bf\\14eed6eafd0a83f76eab5cf8eb50ddc0b037f059eec2bd2e4a\n",
      "Successfully built kss pecab\n",
      "Installing collected packages: emoji, tomli, pluggy, iniconfig, pytest, pecab, kss\n",
      "Successfully installed emoji-1.2.0 iniconfig-2.0.0 kss-4.5.4 pecab-1.0.8 pluggy-1.3.0 pytest-7.4.3 tomli-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip  install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from seaborn) (1.26.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from seaborn) (1.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (6.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.3->seaborn) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/294.6 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 71.7/294.6 kB 975.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.6/294.6 kB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\echoi\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics \n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ------------\n",
      "absl-py                       2.0.0\n",
      "aiohttp                       3.9.0\n",
      "aiosignal                     1.3.1\n",
      "altair                        4.1.0\n",
      "annotated-types               0.6.0\n",
      "anyio                         3.7.1\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "astor                         0.8.1\n",
      "asttokens                     2.4.0\n",
      "astunparse                    1.6.3\n",
      "async-timeout                 4.0.3\n",
      "attrs                         23.1.0\n",
      "Babel                         2.11.0\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.5\n",
      "base58                        2.1.1\n",
      "beautifulsoup4                4.12.2\n",
      "bert-extractive-summarizer    0.10.1\n",
      "bleach                        4.1.0\n",
      "blinker                       1.7.0\n",
      "blis                          0.7.11\n",
      "boto3                         1.33.6\n",
      "botocore                      1.33.6\n",
      "breadability                  0.1.20\n",
      "brotlipy                      0.7.0\n",
      "cachetools                    5.3.2\n",
      "catalogue                     2.0.10\n",
      "certifi                       2023.7.22\n",
      "cffi                          1.15.1\n",
      "chardet                       5.2.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         8.1.7\n",
      "cloudpathlib                  0.16.0\n",
      "colorama                      0.4.6\n",
      "comm                          0.1.4\n",
      "confection                    0.1.4\n",
      "contourpy                     1.1.1\n",
      "cryptography                  41.0.3\n",
      "cycler                        0.12.1\n",
      "cymem                         2.0.8\n",
      "datasets                      2.15.0\n",
      "DateTime                      5.3\n",
      "debugpy                       1.6.7\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "dill                          0.3.7\n",
      "docopt                        0.6.2\n",
      "easydict                      1.11\n",
      "emoji                         1.2.0\n",
      "entrypoints                   0.4\n",
      "exceptiongroup                1.1.3\n",
      "executing                     1.2.0\n",
      "fastapi                       0.104.1\n",
      "fastjsonschema                2.16.2\n",
      "filelock                      3.9.0\n",
      "flatbuffers                   23.5.26\n",
      "fonttools                     4.43.1\n",
      "frozenlist                    1.4.0\n",
      "fsspec                        2023.10.0\n",
      "gast                          0.5.4\n",
      "gensim                        4.3.2\n",
      "gitdb                         4.0.11\n",
      "GitPython                     3.1.40\n",
      "google-auth                   2.23.4\n",
      "google-auth-oauthlib          1.0.0\n",
      "google-pasta                  0.2.0\n",
      "grpcio                        1.59.2\n",
      "h11                           0.14.0\n",
      "h5py                          3.10.0\n",
      "httptools                     0.6.1\n",
      "huggingface-hub               0.17.3\n",
      "idna                          3.4\n",
      "importlib-metadata            6.8.0\n",
      "importlib-resources           6.1.0\n",
      "iniconfig                     2.0.0\n",
      "ipykernel                     6.25.2\n",
      "ipython                       8.16.1\n",
      "ipython-genutils              0.2.0\n",
      "jedi                          0.19.1\n",
      "Jinja2                        3.1.2\n",
      "jmespath                      1.0.1\n",
      "joblib                        1.3.2\n",
      "JPype1                        1.4.1\n",
      "json5                         0.9.6\n",
      "jsonschema                    4.17.3\n",
      "jupyter_client                8.4.0\n",
      "jupyter_core                  5.4.0\n",
      "jupyter-server                1.23.4\n",
      "jupyterlab                    3.3.2\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab_server             2.22.0\n",
      "keras                         2.14.0\n",
      "kiwisolver                    1.4.5\n",
      "kobert-transformers           0.5.1\n",
      "konlpy                        0.6.0\n",
      "kss                           4.5.4\n",
      "langcodes                     3.3.0\n",
      "libclang                      16.0.6\n",
      "lightning-utilities           0.10.0\n",
      "lxml                          4.9.3\n",
      "Markdown                      3.5.1\n",
      "MarkupSafe                    2.1.1\n",
      "matplotlib                    3.8.0\n",
      "matplotlib-inline             0.1.6\n",
      "mistune                       0.8.4\n",
      "mkl-fft                       1.3.8\n",
      "mkl-random                    1.2.4\n",
      "mkl-service                   2.4.0\n",
      "ml-dtypes                     0.2.0\n",
      "mpmath                        1.3.0\n",
      "multidict                     6.0.4\n",
      "multiprocess                  0.70.15\n",
      "murmurhash                    1.0.10\n",
      "mysql-connector-python        8.2.0\n",
      "mysqlclient                   2.2.0\n",
      "nbclassic                     0.5.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.5.4\n",
      "nbformat                      5.9.2\n",
      "nest-asyncio                  1.5.8\n",
      "networkx                      3.1\n",
      "nltk                          3.8.1\n",
      "notebook_shim                 0.2.2\n",
      "numpy                         1.26.0\n",
      "oauthlib                      3.2.2\n",
      "opt-einsum                    3.3.0\n",
      "packaging                     23.2\n",
      "pandas                        1.2.3\n",
      "pandocfilters                 1.5.0\n",
      "parso                         0.8.3\n",
      "pecab                         1.0.8\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        10.0.1\n",
      "pip                           23.3\n",
      "platformdirs                  3.11.0\n",
      "pluggy                        1.3.0\n",
      "preshed                       3.0.9\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.39\n",
      "protobuf                      4.21.12\n",
      "psutil                        5.9.0\n",
      "pure-eval                     0.2.2\n",
      "pyarrow                       14.0.1\n",
      "pyarrow-hotfix                0.5\n",
      "pyasn1                        0.5.0\n",
      "pyasn1-modules                0.3.0\n",
      "pycountry                     22.3.5\n",
      "pycparser                     2.21\n",
      "pydantic                      2.5.2\n",
      "pydantic_core                 2.14.5\n",
      "pydeck                        0.8.1b0\n",
      "Pygments                      2.16.1\n",
      "PyMySQL                       1.1.0\n",
      "pyOpenSSL                     23.2.0\n",
      "pyparsing                     3.1.1\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pytest                        7.4.3\n",
      "python-dateutil               2.8.2\n",
      "python-dotenv                 1.0.0\n",
      "pytorch-lightning             2.1.2\n",
      "pytz                          2023.3.post1\n",
      "pywin32                       305.1\n",
      "pywinpty                      2.0.10\n",
      "PyYAML                        6.0\n",
      "pyzmq                         23.2.1\n",
      "regex                         2023.10.3\n",
      "requests                      2.31.0\n",
      "requests-oauthlib             1.3.1\n",
      "rouge                         1.0.0\n",
      "rouge-score                   0.1.2\n",
      "rsa                           4.9\n",
      "s3transfer                    0.8.2\n",
      "sacremoses                    0.1.1\n",
      "safetensors                   0.4.0\n",
      "scikit-learn                  1.3.2\n",
      "scipy                         1.11.3\n",
      "seaborn                       0.13.0\n",
      "Send2Trash                    1.8.0\n",
      "sentence-transformers         2.2.2\n",
      "sentencepiece                 0.1.99\n",
      "setuptools                    68.0.0\n",
      "six                           1.16.0\n",
      "smart-open                    6.4.0\n",
      "smmap                         5.0.1\n",
      "sniffio                       1.2.0\n",
      "soupsieve                     2.5\n",
      "spacy                         3.7.2\n",
      "spacy-legacy                  3.0.12\n",
      "spacy-loggers                 1.0.5\n",
      "srsly                         2.4.8\n",
      "stack-data                    0.6.2\n",
      "starlette                     0.27.0\n",
      "streamlit                     0.80.0\n",
      "sumy                          0.8.1\n",
      "sympy                         1.11.1\n",
      "tensorboard                   2.14.1\n",
      "tensorboard-data-server       0.7.2\n",
      "tensorboardX                  2.6.2.2\n",
      "tensorflow                    2.14.0\n",
      "tensorflow-estimator          2.14.0\n",
      "tensorflow-intel              2.14.0\n",
      "tensorflow-io-gcs-filesystem  0.31.0\n",
      "termcolor                     2.3.0\n",
      "terminado                     0.17.1\n",
      "thinc                         8.2.1\n",
      "threadpoolctl                 3.2.0\n",
      "tinycss2                      1.2.1\n",
      "tokenizers                    0.14.1\n",
      "toml                          0.10.2\n",
      "tomli                         2.0.1\n",
      "toolz                         0.12.0\n",
      "torch                         2.1.0\n",
      "torchaudio                    2.1.0\n",
      "torchmetrics                  1.2.1\n",
      "torchvision                   0.16.0\n",
      "tornado                       6.2\n",
      "tqdm                          4.66.1\n",
      "traitlets                     5.11.2\n",
      "transformers                  4.35.2\n",
      "typer                         0.9.0\n",
      "typing_extensions             4.8.0\n",
      "tzdata                        2023.3\n",
      "tzlocal                       5.2\n",
      "urllib3                       1.26.16\n",
      "uvicorn                       0.24.0.post1\n",
      "validators                    0.22.0\n",
      "wasabi                        1.1.2\n",
      "watchdog                      3.0.0\n",
      "watchfiles                    0.21.0\n",
      "wcwidth                       0.2.8\n",
      "weasel                        0.3.4\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "websockets                    12.0\n",
      "Werkzeug                      3.0.1\n",
      "wheel                         0.41.2\n",
      "win-inet-pton                 1.1.0\n",
      "wrapt                         1.14.1\n",
      "xxhash                        3.4.1\n",
      "yarl                          1.9.2\n",
      "zipp                          3.17.0\n",
      "zope.interface                6.1\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKEN_COUNT = 512\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\echoi\\\\project\\\\kpfbertsum'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243983"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = 'data/train_original.json'\n",
    "df = pd.read_json(DATA_TRAIN_PATH)\n",
    "df = df.dropna()\n",
    "len(df)#, len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30122"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TEST_PATH = 'data/valid_original.json'\n",
    "test_df = pd.read_json(DATA_TEST_PATH)\n",
    "test_df = test_df.dropna()\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((231783, 3), (12200, 3), (30122, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.05)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test setting all data downsize\n",
    "# downsize = 2000\n",
    "# train_df = train_df[:downsize]\n",
    "# test_df = test_df[:downsize//10]\n",
    "# val_df = val_df[:downsize//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 3), (200, 3), (200, 3))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    outs = []\n",
    "    for doc in data['documents']:\n",
    "        line = []\n",
    "        line.append(doc['media_name'])\n",
    "        line.append(doc['id'])\n",
    "        para = []\n",
    "        for sent in doc['text']:\n",
    "            for s in sent:\n",
    "                para.append(s['sentence'])\n",
    "        line.append(para)\n",
    "        line.append(doc['abstractive'][0])\n",
    "        line.append(doc['extractive'])\n",
    "        a = doc['extractive']\n",
    "        if a[0] == None or a[1] == None or a[2] == None:\n",
    "            continue\n",
    "        outs.append(line)\n",
    "\n",
    "    outs_df = pd.DataFrame(outs)\n",
    "    outs_df.columns = ['media', 'id', 'article_original', 'abstractive', 'extractive']\n",
    "    return outs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media</th>\n",
       "      <th>id</th>\n",
       "      <th>article_original</th>\n",
       "      <th>abstractive</th>\n",
       "      <th>extractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>중도일보</td>\n",
       "      <td>353559055</td>\n",
       "      <td>[금 2, 은 3, 동 2 획득, 올해 최고 성적인 종합 2위 달성, 제14회 대통...</td>\n",
       "      <td>서산시청 사격팀은 '제14회 대통령경호처장기 전국사격대회에서 금메달 2개, 은메달 ...</td>\n",
       "      <td>[2, 3, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  media         id                                   article_original  \\\n",
       "0  중도일보  353559055  [금 2, 은 3, 동 2 획득, 올해 최고 성적인 종합 2위 달성, 제14회 대통...   \n",
       "\n",
       "                                         abstractive extractive  \n",
       "0  서산시청 사격팀은 '제14회 대통령경호처장기 전국사격대회에서 금메달 2개, 은메달 ...  [2, 3, 4]  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = preprocess_data(train_df)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 본    문 =====\n",
      "0 : 특성화고 80명, 보훈대상자 20명\n",
      "1 : 올 한해 총 750명 채용 계획\n",
      "2 : 우리은행(은행장 손태승)은 일자리 창출과 우수 인재 확보를 위해 100여 명의 특별채용을 진행하고 있다고 23일 밝혔다.\n",
      "3 : 우리은행은 이번 특별채용을 통해 특성화고 출신 80명, 국가보훈대상자 20명을 채용한다.\n",
      "4 : 올해 특성화고 출신 채용 인원은 80명으로 이는 전년의 60명 대비 33% 증가한 규모다.\n",
      "5 : 우리은행은 지난 8년간 금융권 최대 규모인 778명의 특성화고 출신 행원을 채용했다.\n",
      "6 : 특성화고 출신 채용은 교육부와 협업을 통해 학교장 추천을 받은 고3 학생을 대상으로 진행되며, 현재 서류전형 합격자를 대상으로 1차 면접이 진행되고 있다.\n",
      "7 : 올해 보훈 특별채용 인원은 총 40명으로 이는 전년 대비 100% 증가한 규모다.\n",
      "8 : 우리은행은 지난 상반기에 국가보훈대상자 20명을 채용했으며, 하반기 역시 20명을 채용한다.\n",
      "9 : 서류전형 합격자를 대상으로 23일 1차 면접을 진행 중이다.\n",
      "10 : 보훈 특별채용과 관련 손태승 우리금융그룹회장 겸 우리은행장은 지난 7월 서울지방보훈청 주최로 진행된 '2019년 호국보훈의 달 대외 유공인사 국가보훈처장 감사패 전수식'에서 국가보훈처장 감사패를 받았다.\n",
      "11 : 국가보훈처는 매년 6월 호국보훈의 달을 맞아 보훈대상자 취업에 모범이 되는 기관의 장에게 국가보훈처장감사패를 수여한다.\n",
      "12 : 한편, 우리은행은 금융권 고용창출을 위해 올해 750명을 채용할 계획이다.\n",
      "13 : 현재 상반기 300명 규모의 채용을 완료했으며, 하반기에는 본 특별채용을 포함하여 450명을 채용할 계획이다.\n",
      "14 : 우리은행 채용 담당자는 \"직무능력과 인성을 겸비한 신입행원을 선발하기 위하여 블라인드 면접을 진행하고 있다\"며, \"우리은행 취업을 위해 부단히 준비해 온 지원자에게 좋은 결과가 있기를 바란다\"고 말했다.\n",
      "===== 요약정답 =====\n",
      "[2, 12, 14]\n",
      "===== 추출본문 =====\n",
      "1 : 우리은행(은행장 손태승)은 일자리 창출과 우수 인재 확보를 위해 100여 명의 특별채용을 진행하고 있다고 23일 밝혔다.\n",
      "2 : 한편, 우리은행은 금융권 고용창출을 위해 올해 750명을 채용할 계획이다.\n",
      "3 : 우리은행 채용 담당자는 \"직무능력과 인성을 겸비한 신입행원을 선발하기 위하여 블라인드 면접을 진행하고 있다\"며, \"우리은행 취업을 위해 부단히 준비해 온 지원자에게 좋은 결과가 있기를 바란다\"고 말했다.\n",
      "===== 생성본문 =====\n",
      "우리은행은 일자리 창출과 우수 인재 확보를 위해 특성화고 출신 80명, 국가보훈대상자 20명 등 100여 명의 특별채용을 진행하고 있다고 23일 밝혔고 이와 별도로 금융권 고용창출을 위해 올해 750명을 채용할 예정이며 직무능력과 인성을 겸비한 신입행원을 뽑기 위하여 블라인드 면접을 진행하고 있다.\n"
     ]
    }
   ],
   "source": [
    "i = 8\n",
    "print('===== 본    문 =====')\n",
    "for idx, str in enumerate(train_df['article_original'][i]):\n",
    "    print(idx,':',str)\n",
    "print('===== 요약정답 =====')\n",
    "print(train_df['extractive'][i])\n",
    "print('===== 추출본문 =====')\n",
    "print('1 :', train_df['article_original'][i][train_df['extractive'][i][0]])\n",
    "print('2 :', train_df['article_original'][i][train_df['extractive'][i][1]])\n",
    "print('3 :', train_df['article_original'][i][train_df['extractive'][i][2]])\n",
    "print('===== 생성본문 =====')\n",
    "print(train_df['abstractive'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media</th>\n",
       "      <th>id</th>\n",
       "      <th>article_original</th>\n",
       "      <th>abstractive</th>\n",
       "      <th>extractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>한국경제</td>\n",
       "      <td>340626877</td>\n",
       "      <td>[[ 박재원 기자 ] '대한민국 5G 홍보대사'를 자처한 문재인 대통령은 \"넓고, ...</td>\n",
       "      <td>8일 서울에서 열린 5G플러스 전략발표에 참석한 문재인 대통령은 5G는 대한민국 혁...</td>\n",
       "      <td>[0, 1, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  media         id                                   article_original  \\\n",
       "0  한국경제  340626877  [[ 박재원 기자 ] '대한민국 5G 홍보대사'를 자처한 문재인 대통령은 \"넓고, ...   \n",
       "\n",
       "                                         abstractive extractive  \n",
       "0  8일 서울에서 열린 5G플러스 전략발표에 참석한 문재인 대통령은 5G는 대한민국 혁...  [0, 1, 3]  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = preprocess_data(test_df)\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media</th>\n",
       "      <th>id</th>\n",
       "      <th>article_original</th>\n",
       "      <th>abstractive</th>\n",
       "      <th>extractive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>전북일보</td>\n",
       "      <td>351014195</td>\n",
       "      <td>[최명국, 송하진 지사, 방중 주요 성과로 '군산~연운항' 항로 개설 협의 꼽아, ...</td>\n",
       "      <td>송하진 전북도지사가 중국 장쑤성을 방문해 러우 친지앤 당서기와 새만금 산단 5공구 ...</td>\n",
       "      <td>[3, 7, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  media         id                                   article_original  \\\n",
       "0  전북일보  351014195  [최명국, 송하진 지사, 방중 주요 성과로 '군산~연운항' 항로 개설 협의 꼽아, ...   \n",
       "\n",
       "                                         abstractive extractive  \n",
       "0  송하진 전북도지사가 중국 장쑤성을 방문해 러우 친지앤 당서기와 새만금 산단 5공구 ...  [3, 7, 8]  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = preprocess_data(val_df)\n",
    "val_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'kpfbert-base'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data: pd.DataFrame, \n",
    "        tokenizer: BertTokenizer, \n",
    "        max_token_len: int = 512\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_token_len = max_token_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        tokenlist = []\n",
    "        for sent in data_row.article_original:\n",
    "            tokenlist.append(tokenizer(\n",
    "                text = sent,\n",
    "                add_special_tokens = True)) #, # Add '[CLS]' and '[SEP]'\n",
    "    \n",
    "        src = [] # 토크나이징 된 전체 문단\n",
    "        labels = []  # 요약문에 해당하면 1, 아니면 0으로 문장수 만큼 생성\n",
    "        segs = []  #각 토큰에 대해 홀수번째 문장이면 0, 짝수번째 문장이면 1을 매핑\n",
    "        clss = []  #[CLS]토큰의 포지션값을 지정\n",
    "\n",
    "        odd = 0\n",
    "        for tkns in tokenlist:\n",
    "            if odd > 1 : odd = 0\n",
    "            clss = clss + [len(src)]\n",
    "            src = src + tkns['input_ids']\n",
    "            segs = segs + [odd] * len(tkns['input_ids'])\n",
    "            if tokenlist.index(tkns) in data_row.extractive :\n",
    "                labels = labels + [1]\n",
    "            else:\n",
    "                labels = labels + [0]\n",
    "            odd += 1\n",
    "        \n",
    "            #truncation\n",
    "            if len(src) == MAX_TOKEN_COUNT:\n",
    "                break\n",
    "            elif len(src) > MAX_TOKEN_COUNT:\n",
    "                src = src[:self.max_token_len - 1] + [src[-1]]\n",
    "                segs = segs[:self.max_token_len]\n",
    "                break\n",
    "    \n",
    "        #padding\n",
    "        if len(src) < MAX_TOKEN_COUNT:\n",
    "            src = src + [0]*(self.max_token_len - len(src))\n",
    "            segs = segs + [0]*(self.max_token_len - len(segs))\n",
    "            \n",
    "        if len(clss) < MAX_TOKEN_COUNT:\n",
    "            clss = clss + [-1]*(self.max_token_len - len(clss))\n",
    "        if len(labels) < MAX_TOKEN_COUNT:\n",
    "            labels = labels + [0]*(self.max_token_len - len(labels))\n",
    "\n",
    "        return dict(\n",
    "            src = torch.tensor(src),\n",
    "            segs = torch.tensor(segs),\n",
    "            clss = torch.tensor(clss),\n",
    "            labels= torch.FloatTensor(labels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_df, test_df, val_df, tokenizer, batch_size=1, max_token_len=512):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.val_df = val_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = SummDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "        self.test_dataset = SummDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "    \n",
    "        self.val_dataset = SummDataset(\n",
    "            self.val_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0 # windows는 0으로 고정해야 에러 안난다. num_workers=2\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0 # windows는 0으로 고정해야 에러 안난다. num_workers=2\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0 # windows는 0으로 고정해야 에러 안난다. num_workers=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = SummDataModule(\n",
    "  train_df,\n",
    "  test_df,  \n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout, dim, max_len=5000):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                              -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb, step=None):\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        if (step):\n",
    "            emb = emb + self.pe[:, step][:, None, :]\n",
    "\n",
    "        else:\n",
    "            emb = emb + self.pe[:, :emb.size(1)]\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n",
    "\n",
    "    def get_emb(self, emb):\n",
    "        return self.pe[:, :emb.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadedAttention(\n",
    "            heads, d_model, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, iter, query, inputs, mask):\n",
    "        if (iter != 0):\n",
    "            input_norm = self.layer_norm(inputs)\n",
    "        else:\n",
    "            input_norm = inputs\n",
    "\n",
    "        mask = mask.unsqueeze(1)\n",
    "        context = self.self_attn(input_norm, input_norm, input_norm,\n",
    "                                 mask=mask)\n",
    "        out = self.dropout(context) + inputs\n",
    "        return self.feed_forward(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtTransformerEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, d_ff=2048, heads=8, dropout=0.2, num_inter_layers=2):\n",
    "        super(ExtTransformerEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_inter_layers = num_inter_layers\n",
    "        self.pos_emb = PositionalEncoding(dropout, hidden_size)\n",
    "        self.transformer_inter = nn.ModuleList(\n",
    "            [TransformerEncoderLayer(hidden_size, heads, d_ff, dropout)\n",
    "            for _ in range(num_inter_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.wo = nn.Linear(hidden_size, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, top_vecs, mask):\n",
    "        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n",
    "\n",
    "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
    "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
    "        x = top_vecs * mask[:, :, None].float()\n",
    "        x = x + pos_emb\n",
    "\n",
    "        for i in range(self.num_inter_layers):\n",
    "            x = self.transformer_inter[i](i, x, x, ~mask) \n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        sent_scores = self.sigmoid(self.wo(x))\n",
    "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
    "\n",
    "        return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\" A two-layer Feed-Forward-Network with residual layer norm.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): the size of input for the first-layer of the FFN.\n",
    "        d_ff (int): the hidden layer size of the second-layer\n",
    "            of the FNN.\n",
    "        dropout (float): dropout probability in :math:`[0, 1)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def gelu(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inter = self.dropout_1(self.gelu(self.w_1(self.layer_norm(x))))\n",
    "        output = self.dropout_2(self.w_2(inter))\n",
    "        return output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module from\n",
    "    \"Attention is All You Need\"\n",
    "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n",
    "\n",
    "    Similar to standard `dot` attention but uses\n",
    "    multiple attention distributions simulataneously\n",
    "    to select relevant items.\n",
    "\n",
    "    .. mermaid::\n",
    "\n",
    "       graph BT\n",
    "          A[key]\n",
    "          B[value]\n",
    "          C[query]\n",
    "          O[output]\n",
    "          subgraph Attn\n",
    "            D[Attn 1]\n",
    "            E[Attn 2]\n",
    "            F[Attn N]\n",
    "          end\n",
    "          A --> D\n",
    "          C --> D\n",
    "          A --> E\n",
    "          C --> E\n",
    "          A --> F\n",
    "          C --> F\n",
    "          D --> O\n",
    "          E --> O\n",
    "          F --> O\n",
    "          B --> O\n",
    "\n",
    "    Also includes several additional tricks.\n",
    "\n",
    "    Args:\n",
    "       head_count (int): number of parallel heads\n",
    "       model_dim (int): the dimension of keys/values/queries,\n",
    "           must be divisible by head_count\n",
    "       dropout (float): dropout parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
    "        assert model_dim % head_count == 0\n",
    "        self.dim_per_head = model_dim // head_count\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_count = head_count\n",
    "\n",
    "        self.linear_keys = nn.Linear(model_dim,\n",
    "                                     head_count * self.dim_per_head)\n",
    "        self.linear_values = nn.Linear(model_dim,\n",
    "                                       head_count * self.dim_per_head)\n",
    "        self.linear_query = nn.Linear(model_dim,\n",
    "                                      head_count * self.dim_per_head)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.use_final_linear = use_final_linear\n",
    "        if (self.use_final_linear):\n",
    "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, key, value, query, mask=None,\n",
    "                layer_cache=None, type=None, predefined_graph_1=None):\n",
    "        \"\"\"\n",
    "        Compute the context vector and the attention vectors.\n",
    "\n",
    "        Args:\n",
    "           key (`FloatTensor`): set of `key_len`\n",
    "                key vectors `[batch, key_len, dim]`\n",
    "           value (`FloatTensor`): set of `key_len`\n",
    "                value vectors `[batch, key_len, dim]`\n",
    "           query (`FloatTensor`): set of `query_len`\n",
    "                 query vectors  `[batch, query_len, dim]`\n",
    "           mask: binary mask indicating which keys have\n",
    "                 non-zero attention `[batch, query_len, key_len]`\n",
    "        Returns:\n",
    "           (`FloatTensor`, `FloatTensor`) :\n",
    "\n",
    "           * output context vectors `[batch, query_len, dim]`\n",
    "           * one of the attention vectors `[batch, query_len, key_len]`\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = key.size(0)\n",
    "        dim_per_head = self.dim_per_head\n",
    "        head_count = self.head_count\n",
    "        key_len = key.size(1)\n",
    "        query_len = query.size(1)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"  projection \"\"\"\n",
    "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
    "                .transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"  compute context \"\"\"\n",
    "            return x.transpose(1, 2).contiguous() \\\n",
    "                .view(batch_size, -1, head_count * dim_per_head)\n",
    "\n",
    "        # 1) Project key, value, and query.\n",
    "        if layer_cache is not None:\n",
    "            if type == \"self\":\n",
    "                query, key, value = self.linear_query(query), \\\n",
    "                                    self.linear_keys(query), \\\n",
    "                                    self.linear_values(query)\n",
    "\n",
    "                key = shape(key)\n",
    "                value = shape(value)\n",
    "\n",
    "                if layer_cache is not None:\n",
    "                    device = key.device\n",
    "                    if layer_cache[\"self_keys\"] is not None:\n",
    "                        key = torch.cat(\n",
    "                            (layer_cache[\"self_keys\"].to(device), key),\n",
    "                            dim=2)\n",
    "                    if layer_cache[\"self_values\"] is not None:\n",
    "                        value = torch.cat(\n",
    "                            (layer_cache[\"self_values\"].to(device), value),\n",
    "                            dim=2)\n",
    "                    layer_cache[\"self_keys\"] = key\n",
    "                    layer_cache[\"self_values\"] = value\n",
    "            elif type == \"context\":\n",
    "                query = self.linear_query(query)\n",
    "                if layer_cache is not None:\n",
    "                    if layer_cache[\"memory_keys\"] is None:\n",
    "                        key, value = self.linear_keys(key), \\\n",
    "                                     self.linear_values(value)\n",
    "                        key = shape(key)\n",
    "                        value = shape(value)\n",
    "                    else:\n",
    "                        key, value = layer_cache[\"memory_keys\"], \\\n",
    "                                     layer_cache[\"memory_values\"]\n",
    "                    layer_cache[\"memory_keys\"] = key\n",
    "                    layer_cache[\"memory_values\"] = value\n",
    "                else:\n",
    "                    key, value = self.linear_keys(key), \\\n",
    "                                 self.linear_values(value)\n",
    "                    key = shape(key)\n",
    "                    value = shape(value)\n",
    "        else:\n",
    "            key = self.linear_keys(key)\n",
    "            value = self.linear_values(value)\n",
    "            query = self.linear_query(query)\n",
    "            key = shape(key)\n",
    "            value = shape(value)\n",
    "\n",
    "        query = shape(query)\n",
    "\n",
    "        key_len = key.size(2)\n",
    "        query_len = query.size(2)\n",
    "\n",
    "        # 2) Calculate and scale scores.\n",
    "        query = query / math.sqrt(dim_per_head)\n",
    "        scores = torch.matmul(query, key.transpose(2, 3))\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(scores)\n",
    "            scores = scores.masked_fill(mask, -1e18) # how can i fix it to use fp16...\n",
    "\n",
    "        # 3) Apply attention dropout and compute context vectors.\n",
    "\n",
    "        attn = self.softmax(scores)\n",
    "\n",
    "        if (not predefined_graph_1 is None):\n",
    "            attn_masked = attn[:, -1] * predefined_graph_1\n",
    "            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
    "\n",
    "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
    "\n",
    "        drop_attn = self.dropout(attn)\n",
    "        if (self.use_final_linear):\n",
    "            context = unshape(torch.matmul(drop_attn, value))\n",
    "            output = self.final_linear(context)\n",
    "            return output\n",
    "        else:\n",
    "            context = torch.matmul(drop_attn, value)\n",
    "            return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\echoi\\project\\kpfbertsum\\kpfbert_summary.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/echoi/project/kpfbertsum/kpfbert_summary.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSummarizer\u001b[39;00m(pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echoi/project/kpfbertsum/kpfbert_summary.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, n_training_steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, n_warmup_steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/echoi/project/kpfbertsum/kpfbert_summary.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "class Summarizer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.max_pos = 512\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME) #, return_dict=True)\n",
    "        \n",
    "         # BERT 레이어의 파라미터 동결하기\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False  # BERT 파라미터를 동결함\n",
    "\n",
    "        self.ext_layer = ExtTransformerEncoder()\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.loss = nn.BCELoss(reduction='none')\n",
    "\n",
    "        \n",
    "\n",
    "        for p in self.ext_layer.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, segs, clss, labels=None): #, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "\n",
    "        mask_src = ~(src == 0) #1 - (src == 0)\n",
    "        mask_cls = ~(clss == -1) #1 - (clss == -1)\n",
    "\n",
    "        top_vec = self.bert(src, token_type_ids=segs, attention_mask=mask_src)\n",
    "        top_vec = top_vec.last_hidden_state\n",
    "        \n",
    "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
    "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
    "\n",
    "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
    "        \n",
    "        \n",
    "\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.loss(sent_scores, labels)\n",
    "            \n",
    "            loss = (loss * mask_cls.float()).sum() / len(labels)\n",
    "            \n",
    "            # print(\"sent_scores shape:\", sent_scores.shape)\n",
    "            # print(\"sent_scores dtype:\", sent_scores.dtype)\n",
    "            # print(\"labels shape:\", labels.shape)\n",
    "            # print(\"labels dtype:\", labels.dtype)\n",
    "            print(\"loss:\", loss)\n",
    "            # print(\"loss shape:\", loss.shape)\n",
    "            # print(\"loss dtype:\", loss.dtype)\n",
    "        return loss, sent_scores\n",
    "    \n",
    "    def step(self, batch):\n",
    "\n",
    "        src = batch['src']\n",
    "        if len(batch['labels']) > 0 :\n",
    "            labels = batch['labels']\n",
    "        else:\n",
    "            labels = None\n",
    "        segs = batch['segs']\n",
    "        clss = batch['clss']\n",
    "        \n",
    "        loss, sent_scores = self(src, segs, clss, labels)    \n",
    "        \n",
    "        return loss, sent_scores, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        loss, sent_scores, labels = self.step(batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        \n",
    "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
    "\n",
    "    def acc_loss(self, outputs):\n",
    "        total_loss = 0\n",
    "        hit_cnt = 0\n",
    "        for outp in outputs:\n",
    "            labels = outp['labels'].cpu()\n",
    "            predictions, idxs = outp['predictions'].cpu().sort()\n",
    "            loss = outp['loss'].cpu()\n",
    "            for label, idx in zip(labels, idxs):\n",
    "                for i in range(1,3):\n",
    "                    if label[idx[-i-1]] == 1 : \n",
    "                        hit_cnt += 1\n",
    "\n",
    "            total_loss += loss\n",
    "            \n",
    "        avg_loss = total_loss / len(outputs)\n",
    "        acc = hit_cnt / (3*len(outputs)*len(labels))\n",
    "        \n",
    "        return acc, avg_loss\n",
    "        \n",
    "    def training_epoch_end(self, outputs):\n",
    "        \n",
    "        acc, avg_loss = self.acc_loss(outputs)\n",
    "        \n",
    "        print('acc:', acc, 'avg_loss:', avg_loss)\n",
    "        \n",
    "        self.log('avg_train_loss', avg_loss, prog_bar=True, logger=True)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \n",
    "        acc, avg_loss = self.acc_loss(outputs)\n",
    "        \n",
    "        print('val_acc:', acc, 'avg_val_loss:', avg_loss)\n",
    "        \n",
    "        self.log('avg_val_loss', avg_loss, prog_bar=True, logger=True)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        \n",
    "        acc, avg_loss = self.acc_loss(outputs)\n",
    "        \n",
    "        print('test_acc:', acc, 'avg_test_loss:', avg_loss)\n",
    "        \n",
    "        self.log('avg_test_loss', avg_loss, prog_bar=True, logger=True)\n",
    "\n",
    "        return\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "        steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "        total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=steps_per_epoch,\n",
    "            num_training_steps=total_training_steps\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name_or_path = \"kpfbert-base\"  # Bert 바이너리가 포함된 디렉토리\n",
    "\n",
    "model = BertModel.from_pretrained(model_name_or_path, add_pooling_layer=False)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델 파라미터 초기화\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at kpfbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lightning_logs/\n",
    "!rm -rf checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 58573), started 1:29:26 ago. (Use '!kill 58573' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8b9d2434e465e150\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8b9d2434e465e150\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"avg_val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"kpfBERT_Summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='avg_val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=0,\n",
    "#     precision=16, #소스 수정 또는 패키지 재설치 필요... 런타임 에러.\n",
    "    progress_bar_refresh_rate=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                  | Params\n",
      "----------------------------------------------------\n",
      "0 | bert      | BertModel             | 114 M \n",
      "1 | ext_layer | ExtTransformerEncoder | 11.0 M\n",
      "2 | loss      | BCELoss               | 0     \n",
      "----------------------------------------------------\n",
      "125 M     Trainable params\n",
      "0         Non-trainable params\n",
      "125 M     Total params\n",
      "500.230   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n",
      "sent_scores shape: torch.Size([4, 512])\n",
      "sent_scores dtype: torch.float32\n",
      "labels shape: torch.Size([4, 512])\n",
      "labels dtype: torch.float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb 셀 36\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data_module)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:499\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_dispatch()\n\u001b[1;32m    498\u001b[0m \u001b[39m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch()\n\u001b[1;32m    501\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_dispatch()\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:546\u001b[0m, in \u001b[0;36mTrainer.dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m    545\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:73\u001b[0m, in \u001b[0;36mAccelerator.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer):\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(trainer)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:114\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m'\u001b[39m\u001b[39mTrainer\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_train()\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:607\u001b[0m, in \u001b[0;36mTrainer.run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar_callback\u001b[39m.\u001b[39mdisable()\n\u001b[0;32m--> 607\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_sanity_check(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n\u001b[1;32m    609\u001b[0m \u001b[39m# set stage for logging\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_running_stage(RunningStage\u001b[39m.\u001b[39mTRAINING, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:864\u001b[0m, in \u001b[0;36mTrainer.run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_sanity_check_start()\n\u001b[1;32m    863\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m _, eval_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_evaluation(max_batches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_sanity_val_batches)\n\u001b[1;32m    866\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_sanity_check_end()\n\u001b[1;32m    867\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_sanity_check \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:726\u001b[0m, in \u001b[0;36mTrainer.run_evaluation\u001b[0;34m(self, max_batches, on_epoch)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mevaluation_step_and_end\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 726\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluation_loop\u001b[39m.\u001b[39;49mevaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    727\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\u001b[39m.\u001b[39mevaluation_step_end(output)\n\u001b[1;32m    729\u001b[0m \u001b[39m# hook + store predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py:166\u001b[0m, in \u001b[0;36mEvaluationLoop.evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    164\u001b[0m     model_ref\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 166\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mvalidation_step(args)\n\u001b[1;32m    168\u001b[0m \u001b[39m# capture any logged information\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlogger_connector\u001b[39m.\u001b[39mcache_logged_metrics()\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py:177\u001b[0m, in \u001b[0;36mAccelerator.validation_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    174\u001b[0m args[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m batch\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:131\u001b[0m, in \u001b[0;36mTrainingTypePlugin.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb 셀 36\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     loss, sent_scores, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, loss, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, logger\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: loss, \u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m: sent_scores, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m: labels}\n",
      "\u001b[1;32m/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb 셀 36\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m segs \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39msegs\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m clss \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mclss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m loss, sent_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(src, segs, clss, labels)    \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss, sent_scores, labels\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb 셀 36\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels shape:\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels dtype:\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss, sent_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(src, segs, clss, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 손실 값의 형태 및 타입 출력\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss value:\u001b[39m\u001b[39m\"\u001b[39m, loss)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb 셀 36\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels shape:\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels dtype:\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss, sent_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(src, segs, clss, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 손실 값의 형태 및 타입 출력\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss value:\u001b[39m\u001b[39m\"\u001b[39m, loss)\n",
      "    \u001b[0;31m[... skipping similar frames: Module._call_impl at line 1527 (65 times), Module._wrapped_call_impl at line 1518 (65 times), Summarizer.forward at line 43 (64 times)]\u001b[0m\n",
      "\u001b[1;32m/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb 셀 36\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels shape:\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels dtype:\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss, sent_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(src, segs, clss, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# 손실 값의 형태 및 타입 출력\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss value:\u001b[39m\u001b[39m\"\u001b[39m, loss)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb 셀 36\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m mask_src \u001b[39m=\u001b[39m \u001b[39m~\u001b[39m(src \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m#1 - (src == 0)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m mask_cls \u001b[39m=\u001b[39m \u001b[39m~\u001b[39m(clss \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#1 - (clss == -1)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m top_vec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(src, token_type_ids\u001b[39m=\u001b[39;49msegs, attention_mask\u001b[39m=\u001b[39;49mmask_src)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m top_vec \u001b[39m=\u001b[39m top_vec\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X45sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m sents_vec \u001b[39m=\u001b[39m top_vec[torch\u001b[39m.\u001b[39marange(top_vec\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), clss]\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/kpfbertsum/myenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:365\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == False:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb 셀 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chelsea/kpfbertsum/kpfbert_summary.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sent_scores\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_scores' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
